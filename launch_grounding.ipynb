{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030b5136",
   "metadata": {},
   "source": [
    "# Labeling raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de2849",
   "metadata": {},
   "source": [
    "#### We will use `owlv2` zero-shot classifier from Google to label our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a15920",
   "metadata": {},
   "source": [
    "#### Assuming there is a folder in a root dir with individual images extracted from original .mkv videos (with ffmpeg for example), set it's name to `PIGS_FOLDER_PATH` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81d9531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shace_linux/projects/itmo_cv_course/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "from torchvision.ops import nms\n",
    "from transformers import AutoProcessor, Owlv2ForObjectDetection\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad757688",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIGS_FOLDER_PATH = \"extacted_frames\"\n",
    "MODEL_LABELED_IMGS_PATH = \"model_bboxed_frames\"\n",
    "MODEL_LABELS = \"model_frames_labels\"\n",
    "\n",
    "GROUNDING_PROMPT = [\"individual pig\", \"man view from above\"]\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/owlv2-base-patch16-ensemble\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = Owlv2ForObjectDetection.from_pretrained(model_id).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cc0b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch inference + saving predictions in YOLO format\n",
    "def batch_predict(imgs_path):\n",
    "    images = [Image.open(img).convert(\"RGB\") for img in imgs_path]\n",
    "\n",
    "    inputs = processor(\n",
    "        images=images, text=[GROUNDING_PROMPT] * len(images), return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    target_sizes = torch.Tensor([img.size for img in images])\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs=outputs, threshold=0.3, target_sizes=target_sizes\n",
    "    )\n",
    "\n",
    "    os.makedirs(MODEL_LABELED_IMGS_PATH, exist_ok=True)\n",
    "    os.makedirs(MODEL_LABELS, exist_ok=True)\n",
    "\n",
    "    text_queries = GROUNDING_PROMPT\n",
    "    for image, result, img_path in zip(images, results, imgs_path):\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        boxes = result[\"boxes\"]\n",
    "        scores = result[\"scores\"]\n",
    "        labels = result[\"labels\"]\n",
    "\n",
    "        if len(boxes) > 0:\n",
    "            keep = nms(boxes, scores, iou_threshold=0.3)\n",
    "            boxes = boxes[keep]\n",
    "            scores = scores[keep]\n",
    "            labels = labels[keep]\n",
    "\n",
    "        img_width, img_height = image.size\n",
    "        anno_lines = []\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            if score > 0.3:\n",
    "                box = [round(i) for i in box.tolist()]\n",
    "                draw.rectangle(box, outline=\"blue\", width=3)\n",
    "                label_text = f\"{text_queries[label]} {score:.2f}\"\n",
    "                draw.text((box[0], box[1]), label_text, fill=\"blue\")\n",
    "\n",
    "                # YOLO: [class_id, x_center, y_center, width, height]\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                width = x_max - x_min\n",
    "                height = y_max - y_min\n",
    "                x_center = x_min + width / 2\n",
    "                y_center = y_min + height / 2\n",
    "                x_center_norm = x_center / img_width\n",
    "                y_center_norm = y_center / img_height\n",
    "                width_norm = width / img_width\n",
    "                height_norm = height / img_height\n",
    "                anno_lines.append(\n",
    "                    f\"0 {x_center_norm:.6f} {y_center_norm:.6f} {width_norm:.6f} {height_norm:.6f}\"\n",
    "                )\n",
    "\n",
    "        output_path = os.path.join(\n",
    "            MODEL_LABELED_IMGS_PATH, f\"output_{os.path.basename(img_path)}\"\n",
    "        )\n",
    "        image.save(output_path)\n",
    "\n",
    "        anno_path = os.path.join(\n",
    "            MODEL_LABELS, f\"{os.path.splitext(os.path.basename(img_path))[0]}.txt\"\n",
    "        )\n",
    "        with open(anno_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(anno_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "643d79a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 753/753 [16:20<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "all_imgs = glob(f\"{PIGS_FOLDER_PATH}/*.jpg\")\n",
    "for idx in tqdm(range(0, len(all_imgs), BATCH_SIZE)):\n",
    "    curr_imgs = all_imgs[idx: idx + BATCH_SIZE]\n",
    "\n",
    "    batch_predict(curr_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedf387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3a77b4a",
   "metadata": {},
   "source": [
    "### Splitting into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e4088cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(\n",
    "    output_dir=\"owlv2_preds\", anno_dir=\"owlv2_annotations\", train_ratio=0.8, seed=42\n",
    "):\n",
    "    random.seed(seed)\n",
    "\n",
    "    img_files = glob(os.path.join(output_dir, \"*.jpg\"))\n",
    "    img_basenames = [os.path.splitext(os.path.basename(f))[0] for f in img_files]\n",
    "\n",
    "    anno_files = glob(os.path.join(anno_dir, \"*.txt\"))\n",
    "    anno_basenames = [os.path.splitext(os.path.basename(f))[0] for f in anno_files]\n",
    "    assert set(img_basenames) == set(\n",
    "        anno_basenames\n",
    "    ), \"Mismatch between images and annotations\"\n",
    "\n",
    "    random.shuffle(img_files)\n",
    "    train_size = int(len(img_files) * train_ratio)\n",
    "    train_imgs = img_files[:train_size]\n",
    "    test_imgs = img_files[train_size:]\n",
    "\n",
    "    train_img_dir = os.path.join(output_dir, \"train\")\n",
    "    test_img_dir = os.path.join(output_dir, \"test\")\n",
    "    train_anno_dir = os.path.join(anno_dir, \"train\")\n",
    "    test_anno_dir = os.path.join(anno_dir, \"test\")\n",
    "\n",
    "    os.makedirs(train_img_dir, exist_ok=True)\n",
    "    os.makedirs(test_img_dir, exist_ok=True)\n",
    "    os.makedirs(train_anno_dir, exist_ok=True)\n",
    "    os.makedirs(test_anno_dir, exist_ok=True)\n",
    "\n",
    "    def move_files(img_list, img_dest_dir, anno_dest_dir):\n",
    "        for img_path in img_list:\n",
    "            img_basename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "            anno_path = os.path.join(anno_dir, f\"{img_basename}.txt\")\n",
    "\n",
    "            shutil.move(\n",
    "                img_path, os.path.join(img_dest_dir, os.path.basename(img_path))\n",
    "            )\n",
    "            if os.path.exists(anno_path):\n",
    "                shutil.move(\n",
    "                    anno_path, os.path.join(anno_dest_dir, f\"{img_basename}.txt\")\n",
    "                )\n",
    "\n",
    "    move_files(train_imgs, train_img_dir, train_anno_dir)\n",
    "    print(f\"Moved {len(train_imgs)} images and annotations to train folders\")\n",
    "\n",
    "    move_files(test_imgs, test_img_dir, test_anno_dir)\n",
    "    print(f\"Moved {len(test_imgs)} images and annotations to test folders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "119d5398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 2709 images and annotations to train folders\n",
      "Moved 301 images and annotations to test folders\n"
     ]
    }
   ],
   "source": [
    "split_train_test(\n",
    "    output_dir=PIGS_FOLDER_PATH, anno_dir=MODEL_LABELS, train_ratio=0.9, seed=137\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f356f8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
